import { NextResponse } from 'next/server';

import { getBaseUrl } from '@/lib/urls/urls';

function buildRobots(): string {
  const baseUrl = getBaseUrl().replace(/\/+$/, '');

  return [
    '# Robots rules tuned for search + LLM crawlers',
    'User-Agent: *',
    'Allow: /',
    'Disallow: /api/',
    'Disallow: /_next/',
    'Disallow: /static/',
    'Disallow: /404',
    'Disallow: /500',
    'Disallow: /*.json$',
    '',
    // Common LLM crawlers - allow full site (except protected paths) and llms files
    'User-Agent: GPTBot',
    'Allow: /',
    'Disallow: /api/',
    'Disallow: /_next/',
    'Disallow: /static/',
    'Disallow: /404',
    'Disallow: /500',
    'Disallow: /*.json$',
    'Allow: /llms.txt',
    'Allow: /llms-full.txt',
    '',
    'User-Agent: Claude-Web',
    'Allow: /',
    'Disallow: /api/',
    'Disallow: /_next/',
    'Disallow: /static/',
    'Disallow: /404',
    'Disallow: /500',
    'Disallow: /*.json$',
    'Allow: /llms.txt',
    'Allow: /llms-full.txt',
    '',
    'User-Agent: Anthropic-AI',
    'Allow: /',
    'Disallow: /api/',
    'Disallow: /_next/',
    'Disallow: /static/',
    'Disallow: /404',
    'Disallow: /500',
    'Disallow: /*.json$',
    'Allow: /llms.txt',
    'Allow: /llms-full.txt',
    '',
    'User-Agent: PerplexityBot',
    'Allow: /',
    'Disallow: /api/',
    'Disallow: /_next/',
    'Disallow: /static/',
    'Disallow: /404',
    'Disallow: /500',
    'Disallow: /*.json$',
    'Allow: /llms.txt',
    'Allow: /llms-full.txt',
    '',
    'User-Agent: GoogleOther',
    'Allow: /',
    'Disallow: /api/',
    'Disallow: /_next/',
    'Disallow: /static/',
    'Disallow: /404',
    'Disallow: /500',
    'Disallow: /*.json$',
    'Allow: /llms.txt',
    'Allow: /llms-full.txt',
    '',
    'User-Agent: DuckAssistBot',
    'Allow: /',
    'Disallow: /api/',
    'Disallow: /_next/',
    'Disallow: /static/',
    'Disallow: /404',
    'Disallow: /500',
    'Disallow: /*.json$',
    'Allow: /llms.txt',
    'Allow: /llms-full.txt',
    '',
    // Additional AI/preview crawlers to future-proof coverage
    'User-Agent: FacebookBot',
    'Allow: /',
    'Disallow: /api/',
    'Disallow: /_next/',
    'Disallow: /static/',
    'Disallow: /404',
    'Disallow: /500',
    'Disallow: /*.json$',
    'Allow: /llms.txt',
    'Allow: /llms-full.txt',
    '',
    'User-Agent: Applebot-Extended',
    'Allow: /',
    'Disallow: /api/',
    'Disallow: /_next/',
    'Disallow: /static/',
    'Disallow: /404',
    'Disallow: /500',
    'Disallow: /*.json$',
    'Allow: /llms.txt',
    'Allow: /llms-full.txt',
    '',
    'User-Agent: Googlebot',
    'Allow: /',
    'Disallow: /api/',
    'Disallow: /_next/',
    'Disallow: /static/',
    'Disallow: /404',
    'Disallow: /500',
    'Disallow: /*.json$',
    'Allow: /llms.txt',
    'Allow: /llms-full.txt',
    '',
    `LLM-Content: ${baseUrl}/llms.txt`,
    `LLM-Full-Content: ${baseUrl}/llms-full.txt`,
    `Sitemap: ${baseUrl}/sitemap.xml`,
  ].join('\n');
}

export function GET() {
  const body = buildRobots();
  return new NextResponse(body, {
    status: 200,
    headers: {
      'Content-Type': 'text/plain; charset=utf-8',
      'Cache-Control': 'public, max-age=86400, immutable',
    },
  });
}
